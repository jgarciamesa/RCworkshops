{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Python Machine Learning 3rd Edition* by [Sebastian Raschka](https://sebastianraschka.com) & [Vahid Mirjalili](http://vahidmirjalili.com), Packt Publishing Ltd. 2019\n",
    "\n",
    "Code Repository: https://github.com/rasbt/python-machine-learning-book-3rd-edition\n",
    "\n",
    "Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Classifying Images with Deep Convolutional Neural Networks (Part 2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebastian Raschka & Vahid Mirjalili \n",
    "last updated: 2019-11-03 \n",
    "\n",
    "numpy 1.17.3\n",
    "scipy 1.3.1\n",
    "matplotlib 3.1.1\n",
    "tensorflow 2.0.0\n",
    "tensorflow_datasets 1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!uptime\n",
    "!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender classification from face images using CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "celeba_bldr.download_and_prepare()\n",
    "celeba = celeba_bldr.as_dataset(shuffle_files=False)\n",
    "print(celeba.keys())\n",
    "\n",
    "celeba_train = celeba['train']\n",
    "celeba_valid = celeba['validation']\n",
    "celeba_test = celeba['test']\n",
    "\n",
    "def count_items(ds):\n",
    "    n = 0\n",
    "    for _ in ds:\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "print('Train set:  {}'.format(count_items(celeba_train)))\n",
    "print('Validation: {}'.format(count_items(celeba_valid)))\n",
    "print('Test set:   {}'.format(count_items(celeba_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_train = celeba_train.take(16000)\n",
    "celeba_valid = celeba_valid.take(1000)\n",
    "\n",
    "print('Train set:  {}'.format(count_items(celeba_train)))\n",
    "print('Validation: {}'.format(count_items(celeba_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transformation and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take 5 examples:\n",
    "examples = []\n",
    "for example in celeba_train.take(5):\n",
    "    examples.append(example['image'])\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8.5),facecolor='#FFCCFFAA')\n",
    "\n",
    "## Column 1: cropping to a bounding-box\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "ax.imshow(examples[0])\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "ax.set_title('Crop to a \\nbounding-box', size=15)\n",
    "img_cropped = tf.image.crop_to_bounding_box(\n",
    "    examples[0], 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "\n",
    "## Column 2: flipping (horizontally)\n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "ax.imshow(examples[1])\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "ax.set_title('Flip (horizontal)', size=15)\n",
    "img_flipped = tf.image.flip_left_right(examples[1])\n",
    "ax.imshow(img_flipped)\n",
    "\n",
    "## Column 3: adjust contrast\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "ax.imshow(examples[2])\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "ax.set_title('Adjust constrast', size=15)\n",
    "img_adj_contrast = tf.image.adjust_contrast(\n",
    "    examples[2], contrast_factor=2)\n",
    "ax.imshow(img_adj_contrast)\n",
    "\n",
    "## Column 4: adjust brightness\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "ax.imshow(examples[3])\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "ax.set_title('Adjust brightness', size=15)\n",
    "img_adj_brightness = tf.image.adjust_brightness(\n",
    "    examples[3], delta=0.3)\n",
    "ax.imshow(img_adj_brightness)\n",
    "\n",
    "## Column 5: cropping from image center \n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "ax.imshow(examples[4])\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "ax.set_title('Centeral crop\\nand resize', size=15)\n",
    "img_center_crop = tf.image.central_crop(\n",
    "    examples[4], 0.7)\n",
    "img_resized = tf.image.resize(\n",
    "    img_center_crop, size=(218, 178))\n",
    "ax.imshow(img_resized.numpy().astype('uint8'))\n",
    "\n",
    "# plt.savefig('figures/15_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 12),facecolor='#FFCCFFAA')\n",
    "\n",
    "for i,example in enumerate(celeba_train.take(3)):\n",
    "    image = example['image']\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+1)\n",
    "    ax.imshow(image)\n",
    "    if i == 0:\n",
    "        ax.set_title('Orig.', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+2)\n",
    "    img_crop = tf.image.random_crop(image, size=(178, 178, 3))\n",
    "    ax.imshow(img_crop)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 1: Random crop', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+3)\n",
    "    img_flip = tf.image.random_flip_left_right(img_crop)\n",
    "    ax.imshow(tf.cast(img_flip, tf.uint8))\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 2: Random flip', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+4)\n",
    "    img_resize = tf.image.resize(img_flip, size=(128, 128))\n",
    "    ax.imshow(tf.cast(img_resize, tf.uint8))\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 3: Resize', size=15)\n",
    "\n",
    "# plt.savefig('figures/15_15.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example, size=(64, 64), mode='train'):\n",
    "    image = example['image']\n",
    "    label = example['attributes']['Male']\n",
    "    if mode == 'train':\n",
    "        image_cropped = tf.image.random_crop(\n",
    "            image, size=(178, 178, 3))\n",
    "        image_resized = tf.image.resize(\n",
    "            image_cropped, size=size)\n",
    "        image_flip = tf.image.random_flip_left_right(\n",
    "            image_resized)\n",
    "        return (image_flip/255.0, tf.cast(label, tf.int32))\n",
    "    \n",
    "    else:\n",
    "        image_cropped = tf.image.crop_to_bounding_box(\n",
    "            image, offset_height=20, offset_width=0,\n",
    "            target_height=178, target_width=178)\n",
    "        image_resized = tf.image.resize(\n",
    "            image_cropped, size=size)\n",
    "        return (image_resized/255.0, tf.cast(label, tf.int32))\n",
    "\n",
    "## testing:\n",
    "#item = next(iter(celeba_train))\n",
    "#preprocess(item, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "ds = celeba_train.shuffle(1000, reshuffle_each_iteration=False)\n",
    "ds = ds.take(2).repeat(5)\n",
    "\n",
    "ds = ds.map(lambda x:preprocess(x, size=(178, 178), mode='train'))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6),facecolor='#FFCCFFAA')\n",
    "for j,example in enumerate(ds):\n",
    "    ax = fig.add_subplot(2, 5, j//2+(j%2)*5+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(example[0])\n",
    "    \n",
    "#plt.savefig('figures/15_16.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "IMAGE_SIZE = (64, 64)\n",
    "steps_per_epoch = np.ceil(16000/BATCH_SIZE)\n",
    "print(steps_per_epoch)\n",
    "\n",
    "ds_train = celeba_train.map(\n",
    "    lambda x: preprocess(x, size=IMAGE_SIZE, mode='train'))\n",
    "ds_train = ds_train.shuffle(buffer_size=BUFFER_SIZE).repeat()\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "\n",
    "ds_valid = celeba_valid.map(\n",
    "    lambda x: preprocess(x, size=IMAGE_SIZE, mode='eval'))\n",
    "ds_valid = ds_valid.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN gender classifier\n",
    "\n",
    "* **Global Average Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/15_13.png', width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        128, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(\n",
    "        256, (3, 3), padding='same', activation='relu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_output_shape(input_shape=(None, 64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.compute_output_shape(input_shape=(None, 64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(1, activation=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "\n",
    "model.build(input_shape=(None, 64, 64, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(ds_train, validation_data=ds_valid, \n",
    "                    epochs=20, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "x_arr = np.arange(len(hist['loss'])) + 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4),facecolor='#FFCCFFAA')\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "\n",
    "#plt.savefig('figures/15_18.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = celeba_test.map(\n",
    "    lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)\n",
    "results = model.evaluate(ds_test, verbose=0)\n",
    "print('Test Acc: {:.2f}%'.format(results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(ds_train, validation_data=ds_valid, \n",
    "                    epochs=30, initial_epoch=20,\n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = history.history\n",
    "x_arr = np.arange(len(hist['loss'] + hist2['loss']))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist['loss']+hist2['loss'], \n",
    "        '-o', label='Train Loss')\n",
    "ax.plot(x_arr, hist['val_loss']+hist2['val_loss'],\n",
    "        '--<', label='Validation Loss')\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist['accuracy']+hist2['accuracy'], \n",
    "        '-o', label='Train Acc.')\n",
    "ax.plot(x_arr, hist['val_accuracy']+hist2['val_accuracy'], \n",
    "        '--<', label='Validation Acc.')\n",
    "ax.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = celeba_test.map(\n",
    "    lambda x:preprocess(x, size=IMAGE_SIZE, mode='eval')).batch(32)\n",
    "results = model.evaluate(ds_test, verbose=0)\n",
    "print('Test Acc: {:.2f}%'.format(results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_test.unbatch().take(10)\n",
    "\n",
    "pred_logits = model.predict(ds.batch(10))\n",
    "probas = tf.sigmoid(pred_logits)\n",
    "probas = probas.numpy().flatten()*100\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7),facecolor='#FFCCFFAA')\n",
    "for j,example in enumerate(ds):\n",
    "    ax = fig.add_subplot(2, 5, j+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(example[0])\n",
    "    if example[1].numpy() == 1:\n",
    "        label='Male'\n",
    "    else:\n",
    "        label = 'Female'\n",
    "    ax.text(\n",
    "        0.5, -0.15, \n",
    "        'GT: {:s}\\nPr(Male)={:.0f}%'.format(label, probas[j]), \n",
    "        size=16, \n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center', \n",
    "        transform=ax.transAxes)\n",
    "    \n",
    "#plt.savefig('figures/figures-15_19.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/celeba-cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "### The effect of initial shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## MNIST dataset\n",
    "#datasets = tfds.load(name='mnist')\n",
    "mnist_bldr = tfds.builder('mnist')\n",
    "mnist_bldr.download_and_prepare()\n",
    "datasets = mnist_bldr.as_dataset(shuffle_files=False)\n",
    "mnist_train_orig, mnist_test_orig = datasets['train'], datasets['test']\n",
    "\n",
    "\n",
    "mnist_train = mnist_train_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "mnist_test = mnist_test_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "mnist_train = mnist_train.shuffle(buffer_size=10000,\n",
    "                                  reshuffle_each_iteration=False)\n",
    "\n",
    "mnist_valid = mnist_train.take(100)#.batch(BATCH_SIZE)\n",
    "mnist_train = mnist_train.skip(100)#.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that count-of-labels in mnist_valid did not stay the same when the dataset is loaded with using Builder and specifying `mnist_bldr.as_dataset(shuffle_files=False)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_labels(ds):\n",
    "    counter = Counter()\n",
    "    for example in ds:\n",
    "        counter.update([example[1].numpy()])\n",
    "    return counter\n",
    "    \n",
    "print('Count of labels:', count_labels(mnist_valid))\n",
    "print('Count of labels:', count_labels(mnist_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## MNIST dataset\n",
    "datasets = tfds.load(name='mnist')\n",
    "#mnist_bldr = tfds.builder('mnist')\n",
    "#mnist_bldr.download_and_prepare()\n",
    "#datasets = mnist_bldr.as_dataset(shuffle_files=False)\n",
    "mnist_train_orig, mnist_test_orig = datasets['train'], datasets['test']\n",
    "\n",
    "\n",
    "mnist_train = mnist_train_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "mnist_test = mnist_test_orig.map(\n",
    "    lambda item: (tf.cast(item['image'], tf.float32)/255.0, \n",
    "                  tf.cast(item['label'], tf.int32)))\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "mnist_train = mnist_train.shuffle(buffer_size=10000,\n",
    "                                  reshuffle_each_iteration=False)\n",
    "\n",
    "mnist_valid = mnist_train.take(100)#.batch(BATCH_SIZE)\n",
    "mnist_train = mnist_train.skip(100)#.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that count-of-labels in mnist_valid did not stay the same when the dataset is loaded with `tfds.load()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_labels(ds):\n",
    "    counter = Counter()\n",
    "    for example in ds:\n",
    "        counter.update([example[1].numpy()])\n",
    "    return counter\n",
    "    \n",
    "print('Count of labels:', count_labels(mnist_valid))\n",
    "print('Count of labels:', count_labels(mnist_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python nb2py.py --input ch15_part2.ipynb --output ch15_part2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0-gpu (Tensorflow)",
   "language": "python",
   "name": "tf2.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
